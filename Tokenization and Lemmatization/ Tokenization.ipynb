{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1466953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b51e24b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "love\n",
      "GFG\n",
      ",\n",
      "what\n",
      "'s\n",
      "you\n",
      "choice\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love GFG, what's you choice?\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8559c",
   "metadata": {},
   "source": [
    "## Adding special case tokenization rules"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f853c2f",
   "metadata": {},
   "source": [
    "Most domains have at least some idiosyncrasies that require custom tokenization rules. This could be very certain expressions, or abbreviations only used in this specific field. Hereâ€™s how to add a special case rule to an existing Tokenizer instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3db8433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gimme', 'that']\n",
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"gimme that\")  # phrase to tokenize\n",
    "print([w.text for w in doc])  # ['gimme', 'that']\n",
    "\n",
    "# Add special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "# Check new tokenization\n",
    "print([w.text for w in nlp(\"gimme that\")])  # ['gim', 'me', 'that']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf0073",
   "metadata": {},
   "source": [
    "## Debugging the tokenizer\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d82fd7e",
   "metadata": {},
   "source": [
    "A working implementation of the pseudo-code above is available for debugging as nlp.tokenizer.explain(text). It returns a list of tuples showing which tokenizer rule or pattern was matched for each token. The tokens produced are identical to nlp.tokenizer() except for whitespace tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c12ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c3e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\"Let's go!\"'''\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4728aab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \\t PREFIX\n",
      "Let \\t SPECIAL-1\n",
      "'s \\t SPECIAL-2\n",
      "go \\t TOKEN\n",
      "! \\t SUFFIX\n",
      "\" \\t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\\\t\", t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb7800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
